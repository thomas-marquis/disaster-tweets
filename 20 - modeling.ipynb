{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f351ef3e-3db6-4c65-848b-73f0a3ec1707",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Détections des catastrophes dans des tweets\n",
    "\n",
    "Compétition Kaggle [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dffba2-1e7d-44aa-b01f-525d57c0c4a3",
   "metadata": {},
   "source": [
    "## TODO liste\n",
    "\n",
    "- [ ] faire un preprocessing sur le text : supprimer les stop-words, la ponctuation, ...\n",
    "- [ ] extraire les hastag comme des features à part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9482ff-4b80-415c-952f-64834fc7dbd6",
   "metadata": {},
   "source": [
    "## Dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03c70c1-1b6e-43b5-8cac-f6ccb76b0456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93d4a5-b501-499a-8b13-031e7b227604",
   "metadata": {},
   "source": [
    "## Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5440a0ad-5b14-4786-846c-1b5e5535bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a39411-976d-47ae-ab67-dc7caf142991",
   "metadata": {},
   "source": [
    "## Modèle baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375199d3-3684-4cff-8e9f-6c7f63749111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59421842, 0.56455572, 0.64082434])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text\"])\n",
    "clf = linear_model.RidgeClassifier()\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4f699-d4e6-425e-b5ca-59640fd241e2",
   "metadata": {},
   "source": [
    "## Baseline avec TFIDF et tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "573838bf-ed5f-4436-8415-c733d269e476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.613861444610996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.6514658 , 0.57735247, 0.53373313, 0.52262774, 0.59643917,\n",
       "       0.64083458, 0.61806656, 0.5625    , 0.71671388, 0.71888112])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class UnSparse(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.toarray()\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorize', TfidfVectorizer()),\n",
    "#     ('un_sparse', UnSparse()),\n",
    "#     ('scaler', StandardScaler()),\n",
    "    ('model_clf', RidgeClassifier()),\n",
    "])\n",
    "\n",
    "corpus  = train_df['text']\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, corpus, train_df[\"target\"], cv=10, scoring=\"f1\")\n",
    "print(np.mean(scores))\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc3fb1-5bb2-4dc7-b216-2da197e40fd4",
   "metadata": {},
   "source": [
    "## Basline avec prétraitement du text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2061e14e-bf52-476f-9de5-35a96b8a4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "import nltk\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "sw = stopwords.words('english') + ['the', 'a', 'and', 'is', 'be', 'will']\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "class CleanupText(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, \n",
    "                 stop_words=None, \n",
    "                 remove_stop_words=True, \n",
    "                 remove_punctuations=True, \n",
    "                 clean_whitespaces=True):\n",
    "        self.stop_words = stop_words or sw\n",
    "        self.remove_stop_words = remove_stop_words\n",
    "        self.remove_punctuations = remove_punctuations\n",
    "        self.clean_whitespaces = clean_whitespaces\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self.clean_text(txt) for txt in X])\n",
    "\n",
    "    def clean_text(\n",
    "        self,\n",
    "        string: str, \n",
    "        punctuations: str = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~|''') -> str:\n",
    "        \"\"\"\n",
    "        A method to clean text \n",
    "        \"\"\"\n",
    "        # Cleaning the urls\n",
    "        string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "        # Cleaning the html elements\n",
    "        string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "        # Removing the punctuations\n",
    "        if self.remove_punctuations:\n",
    "            for x in string.lower(): \n",
    "                if x in punctuations: \n",
    "                    string = string.replace(x, \"\") \n",
    "\n",
    "        # Converting the text to lower\n",
    "        string = string.lower()\n",
    "\n",
    "        # Removing stop words\n",
    "        if self.remove_stop_words:\n",
    "            string = ' '.join([word for word in string.split() if word not in self.stop_words])\n",
    "\n",
    "        # Cleaning the whitespaces\n",
    "        if self.clean_whitespaces:\n",
    "            string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3bd838b0-7459-437c-bf90-b4fd30bab147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_clean__clean_whitespaces': True, 'text_clean__remove_punctuations': False, 'text_clean__remove_stop_words': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5908561245751591"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('text_clean', CleanupText()),\n",
    "    ('vectorize', TfidfVectorizer()),\n",
    "    ('model_clf', RidgeClassifier()),\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {'text_clean__remove_stop_words': [True, False], \n",
    "     'text_clean__remove_punctuations': [True, False], \n",
    "     'text_clean__clean_whitespaces': [True, False]},\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=10, scoring='f1')\n",
    "grid_search.fit(train_df['text'], train_df[\"target\"])\n",
    "print(grid_search.best_params_)\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819748d2-a556-4bad-8e17-522fec3e1451",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f462afca-f6a2-4eec-93db-43cd4213c2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_clf__max_leaf_nodes': None, 'model_clf__n_estimators': 500}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vectorize', TfidfVectorizer()),\n",
    "    ('model_clf', RandomForestClassifier(n_jobs=-1)),\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {'model_clf__n_estimators': [100, 200, 500], 'model_clf__max_leaf_nodes': [64, None]},\n",
    "]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3, scoring='f1')\n",
    "grid_search.fit(train_df['text'], train_df[\"target\"])\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d966b7c6-b90e-4e2a-a113-b3b5432d4f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5680143763271022"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fea6c1-ea6b-4d7c-8ce2-b2ce13432298",
   "metadata": {},
   "source": [
    "## Word embeding\n",
    "\n",
    "from [this Medium post](https://medium.com/analytics-vidhya/text-classification-using-word-embeddings-and-deep-learning-in-python-classifying-tweets-from-6fe644fcfc81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb806bd-e8bd-41e9-9e19-2fe3d7222909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (disasters)",
   "language": "python",
   "name": "disasters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
